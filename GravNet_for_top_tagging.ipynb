{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GravNet for top-tagging\n",
    "\n",
    "The data we will use for these examples are the constituents of jets, large collections particles which have been clustered together by some combination algorithm (such at [anti-k<sub>t</sub>](https://arxiv.org/abs/0802.1189)). Jets can be produced in particle collisions due to multiple reasons, such as the emission of an energetic light-quark or gluon (QCD) and the decay of some heavy particle (e.g. a *top* quark). Our data contains simulations for jets produced by both QCD interactions and hadronic top-quark decays, and our task is to learn to classify the production mechanism of each jet (either QCD - target 0, or top - target 1).\n",
    "\n",
    "For each jet we have up to the first 200 jet constituents (our objects), which have been ordered by decreasing transverse momentum (*p<sub>t</sub>*), and we are provided with the Cartesian 4-momenta for each constituent: *p<sub>x</sub>*, *p<sub>y</sub>*, *p<sub>z</sub>*, and *E*.\n",
    "\n",
    "This dataset is available from here https://desycloud.desy.de/index.php/s/llbX3zpLhazgPJ6, or by running the cell below. Warning, the dataset is quite large ~1.6 GB. A discussion of the dataset is documented here: https://docs.google.com/document/d/1Hcuc6LBxZNX16zjEGeq16DAzspkDC4nDTyjMp1bWHRo/edit, and the reference is \"Deep-learned Top Tagging with a Lorentz Layer\" by A Butter, G Kasieczka, T and M Russell ([arXiv: 1707.08966](https://arxiv.org/abs/1707.08966))\n",
    "\n",
    "For the purpose of this tutorial, to save time and bandwidth (24MB versus >1GB), I've shared a preprocessed subsample of this dataset. The link to which becomes invalid on 18/07/21 to encourage the use of the official source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    os.system('pip install lumin==1.8.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from functools import partial\n",
    "from fastcore.all import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('./data')\n",
    "PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRATCH = False  # Whether to import data from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if SCRATCH:\n",
    "    import h5py\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    os.system(\"wget -O data.zip https://desycloud.desy.de/index.php/s/llbX3zpLhazgPJ6/download?path=%2F&files=train.h5\")\n",
    "    os.system(\"unzip data.zip\")\n",
    "    os.system(\"rm data.zip\")\n",
    "    LOAD = Path('./data')\n",
    "\n",
    "#     LOAD = Path('../lumin/examples/data/top/')\n",
    "    \n",
    "    trn_store = pd.HDFStore(LOAD/'train.h5')\n",
    "\n",
    "    def load_df(store:pd.io.pytables.HDFStore, n_evts:Optional[int]=None, n_const:Optional[int]=None) -> pd.DataFrame:\n",
    "        df = trn_store.select(\"table\",stop=n_evts).drop(columns=['truthE','truthPX','truthPY','truthPZ','ttv'])\n",
    "        if n_const is not None: df.drop(columns=[f for f in df.columns if f != 'is_signal_new' and int(f[f.find('_')+1:]) >= n_const], inplace=True)\n",
    "        rename = {f:f'{f[f.find(\"_\")+1:]}_{f[:f.find(\"_\")].lower()}' for f in df.columns if f != 'is_signal_new'}\n",
    "        rename['is_signal_new'] = 'gen_target'\n",
    "        df.rename(columns=rename, inplace=True)\n",
    "        return df\n",
    "\n",
    "    df = load_df(trn_store, 100000, 15)\n",
    "    train_feats = [f for f in df.columns if f != 'gen_target']\n",
    "\n",
    "    from lumin.data_processing.pre_proc import fit_input_pipe\n",
    "    from lumin.data_processing.hep_proc import to_pt_eta_phi, proc_event, get_vecs\n",
    "\n",
    "    def proc_df(df:pd.DataFrame, train_feats:List[str], preproc:bool=True) -> None:\n",
    "        print('Moving to float32')\n",
    "        df[train_feats] = df[train_feats].values.astype('float32')\n",
    "        print('Replacing zeros')\n",
    "        df[train_feats] = df[train_feats].replace(0.0, np.nan)  # Don't bias preprocessing with non-existent tracks\n",
    "        print('Converting vectors')\n",
    "        for v in get_vecs(train_feats): to_pt_eta_phi(df, v, True)    \n",
    "        print('Processing event')\n",
    "        proc_event(df, fix_phi=True, fix_y=True, fix_z=True, ref_vec_0='0', ref_vec_1='1', use_cartesian=True)\n",
    "        print('Readding 0_py')\n",
    "        df['0_py'] = 0.\n",
    "        if preproc:\n",
    "            print('Preprocessing features')\n",
    "            print('Fitting preproc pipe')\n",
    "            input_pipe = fit_input_pipe(df, train_feats, PATH/'input_pipe')\n",
    "            print('Transforming features')\n",
    "            df[train_feats] = input_pipe.transform(df[train_feats])  # Rescale and shift track momenta values\n",
    "        df[train_feats] = df[train_feats].replace(np.nan, 0.0) # Matrix dat amust not contain NaNs\n",
    "\n",
    "    proc_df(df, train_feats)\n",
    "\n",
    "    from lumin.data_processing.file_proc import df2foldfile\n",
    "\n",
    "    def df2fy(df:pd.DataFrame, train_feats:List[str], n_folds:int=10) -> None:\n",
    "        df2foldfile(df=df, n_folds=n_folds,\n",
    "                    cont_feats=train_feats, cat_feats=[], targ_feats='gen_target',\n",
    "                    savename=PATH/'train', targ_type='int', strat_key='gen_target')\n",
    "\n",
    "    df2fy(df, train_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SCRATCH: \n",
    "    os.system(\"wget -O data.zip https://cernbox.cern.ch/index.php/s/YsKrkmIM6rBcnfG/download\")  # Link expires on 18/07/21\n",
    "    os.system(\"unzip data.zip\")\n",
    "    os.system(\"rm data.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumin.nn.data.fold_yielder import FoldYielder\n",
    "\n",
    "train_fy = FoldYielder(PATH/'train.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 60)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fy[0]['inputs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 datapoints loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_e</th>\n",
       "      <th>0_px</th>\n",
       "      <th>0_py</th>\n",
       "      <th>0_pz</th>\n",
       "      <th>1_e</th>\n",
       "      <th>1_px</th>\n",
       "      <th>1_py</th>\n",
       "      <th>1_pz</th>\n",
       "      <th>2_e</th>\n",
       "      <th>2_px</th>\n",
       "      <th>2_py</th>\n",
       "      <th>2_pz</th>\n",
       "      <th>3_e</th>\n",
       "      <th>3_px</th>\n",
       "      <th>3_py</th>\n",
       "      <th>3_pz</th>\n",
       "      <th>4_e</th>\n",
       "      <th>4_px</th>\n",
       "      <th>4_py</th>\n",
       "      <th>4_pz</th>\n",
       "      <th>5_e</th>\n",
       "      <th>5_px</th>\n",
       "      <th>5_py</th>\n",
       "      <th>5_pz</th>\n",
       "      <th>6_e</th>\n",
       "      <th>6_px</th>\n",
       "      <th>6_py</th>\n",
       "      <th>6_pz</th>\n",
       "      <th>7_e</th>\n",
       "      <th>7_px</th>\n",
       "      <th>7_py</th>\n",
       "      <th>7_pz</th>\n",
       "      <th>8_e</th>\n",
       "      <th>8_px</th>\n",
       "      <th>8_py</th>\n",
       "      <th>8_pz</th>\n",
       "      <th>9_e</th>\n",
       "      <th>9_px</th>\n",
       "      <th>9_py</th>\n",
       "      <th>9_pz</th>\n",
       "      <th>10_e</th>\n",
       "      <th>10_px</th>\n",
       "      <th>10_py</th>\n",
       "      <th>10_pz</th>\n",
       "      <th>11_e</th>\n",
       "      <th>11_px</th>\n",
       "      <th>11_py</th>\n",
       "      <th>11_pz</th>\n",
       "      <th>12_e</th>\n",
       "      <th>12_px</th>\n",
       "      <th>12_py</th>\n",
       "      <th>12_pz</th>\n",
       "      <th>13_e</th>\n",
       "      <th>13_px</th>\n",
       "      <th>13_py</th>\n",
       "      <th>13_pz</th>\n",
       "      <th>14_e</th>\n",
       "      <th>14_px</th>\n",
       "      <th>14_py</th>\n",
       "      <th>14_pz</th>\n",
       "      <th>gen_target</th>\n",
       "      <th>gen_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.481700</td>\n",
       "      <td>-0.132916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.609365</td>\n",
       "      <td>-0.582372</td>\n",
       "      <td>-0.205052</td>\n",
       "      <td>-0.638221</td>\n",
       "      <td>-0.631650</td>\n",
       "      <td>-0.738738</td>\n",
       "      <td>-0.481583</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>-0.705460</td>\n",
       "      <td>-0.437595</td>\n",
       "      <td>0.241105</td>\n",
       "      <td>-0.156129</td>\n",
       "      <td>-0.601133</td>\n",
       "      <td>-0.692038</td>\n",
       "      <td>-0.376905</td>\n",
       "      <td>-0.276365</td>\n",
       "      <td>-0.665841</td>\n",
       "      <td>-0.564877</td>\n",
       "      <td>-0.089998</td>\n",
       "      <td>-0.287887</td>\n",
       "      <td>-0.612187</td>\n",
       "      <td>-0.462814</td>\n",
       "      <td>0.164228</td>\n",
       "      <td>0.021311</td>\n",
       "      <td>-0.604728</td>\n",
       "      <td>-0.449970</td>\n",
       "      <td>0.163731</td>\n",
       "      <td>-0.208286</td>\n",
       "      <td>-0.576435</td>\n",
       "      <td>-0.276393</td>\n",
       "      <td>0.512084</td>\n",
       "      <td>-0.026300</td>\n",
       "      <td>-0.496800</td>\n",
       "      <td>-0.090712</td>\n",
       "      <td>0.892368</td>\n",
       "      <td>-0.308557</td>\n",
       "      <td>-0.436427</td>\n",
       "      <td>-0.084787</td>\n",
       "      <td>0.874494</td>\n",
       "      <td>-0.069496</td>\n",
       "      <td>-0.436210</td>\n",
       "      <td>-0.161092</td>\n",
       "      <td>0.701912</td>\n",
       "      <td>-0.352263</td>\n",
       "      <td>-0.475480</td>\n",
       "      <td>-0.149536</td>\n",
       "      <td>0.625301</td>\n",
       "      <td>-0.295651</td>\n",
       "      <td>-0.384463</td>\n",
       "      <td>-0.079100</td>\n",
       "      <td>0.813110</td>\n",
       "      <td>-0.081188</td>\n",
       "      <td>-0.433435</td>\n",
       "      <td>0.065368</td>\n",
       "      <td>1.084211</td>\n",
       "      <td>-0.184324</td>\n",
       "      <td>-0.407374</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.506164</td>\n",
       "      <td>-0.938812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.184967</td>\n",
       "      <td>0.728438</td>\n",
       "      <td>-0.282945</td>\n",
       "      <td>-0.147106</td>\n",
       "      <td>1.005388</td>\n",
       "      <td>1.254904</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.216655</td>\n",
       "      <td>1.456007</td>\n",
       "      <td>1.731084</td>\n",
       "      <td>0.922723</td>\n",
       "      <td>0.127948</td>\n",
       "      <td>1.778195</td>\n",
       "      <td>2.661758</td>\n",
       "      <td>1.306008</td>\n",
       "      <td>0.319365</td>\n",
       "      <td>2.620614</td>\n",
       "      <td>2.095549</td>\n",
       "      <td>1.242490</td>\n",
       "      <td>-0.494361</td>\n",
       "      <td>2.091407</td>\n",
       "      <td>0.503347</td>\n",
       "      <td>0.067526</td>\n",
       "      <td>0.489542</td>\n",
       "      <td>0.707672</td>\n",
       "      <td>1.442638</td>\n",
       "      <td>0.245026</td>\n",
       "      <td>0.119696</td>\n",
       "      <td>1.624627</td>\n",
       "      <td>0.951661</td>\n",
       "      <td>0.598001</td>\n",
       "      <td>0.525819</td>\n",
       "      <td>1.069554</td>\n",
       "      <td>0.476023</td>\n",
       "      <td>0.167476</td>\n",
       "      <td>-0.132578</td>\n",
       "      <td>0.682129</td>\n",
       "      <td>1.334066</td>\n",
       "      <td>0.174130</td>\n",
       "      <td>0.160250</td>\n",
       "      <td>1.560382</td>\n",
       "      <td>0.793039</td>\n",
       "      <td>0.363780</td>\n",
       "      <td>0.193252</td>\n",
       "      <td>0.978892</td>\n",
       "      <td>0.547456</td>\n",
       "      <td>0.223765</td>\n",
       "      <td>-0.195637</td>\n",
       "      <td>0.758276</td>\n",
       "      <td>0.824664</td>\n",
       "      <td>0.424342</td>\n",
       "      <td>-0.276123</td>\n",
       "      <td>1.008659</td>\n",
       "      <td>0.593994</td>\n",
       "      <td>0.253645</td>\n",
       "      <td>-0.251510</td>\n",
       "      <td>0.808679</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.408961</td>\n",
       "      <td>-0.560417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.193791</td>\n",
       "      <td>-0.468981</td>\n",
       "      <td>-0.675213</td>\n",
       "      <td>-0.255126</td>\n",
       "      <td>-0.189504</td>\n",
       "      <td>0.074884</td>\n",
       "      <td>0.186744</td>\n",
       "      <td>-0.004042</td>\n",
       "      <td>0.193590</td>\n",
       "      <td>0.742352</td>\n",
       "      <td>1.282936</td>\n",
       "      <td>0.019557</td>\n",
       "      <td>0.656395</td>\n",
       "      <td>1.076025</td>\n",
       "      <td>1.831397</td>\n",
       "      <td>0.261748</td>\n",
       "      <td>0.891446</td>\n",
       "      <td>0.944363</td>\n",
       "      <td>1.521067</td>\n",
       "      <td>0.125594</td>\n",
       "      <td>0.830918</td>\n",
       "      <td>-0.012943</td>\n",
       "      <td>-0.049518</td>\n",
       "      <td>0.763610</td>\n",
       "      <td>0.161204</td>\n",
       "      <td>0.173490</td>\n",
       "      <td>0.356810</td>\n",
       "      <td>0.534480</td>\n",
       "      <td>0.267975</td>\n",
       "      <td>0.407054</td>\n",
       "      <td>0.740283</td>\n",
       "      <td>0.336708</td>\n",
       "      <td>0.427715</td>\n",
       "      <td>0.087282</td>\n",
       "      <td>0.285830</td>\n",
       "      <td>-0.100254</td>\n",
       "      <td>0.202652</td>\n",
       "      <td>0.041766</td>\n",
       "      <td>0.137729</td>\n",
       "      <td>1.233500</td>\n",
       "      <td>0.132744</td>\n",
       "      <td>0.188765</td>\n",
       "      <td>0.327576</td>\n",
       "      <td>1.177694</td>\n",
       "      <td>0.251895</td>\n",
       "      <td>0.410545</td>\n",
       "      <td>0.579704</td>\n",
       "      <td>0.129596</td>\n",
       "      <td>0.485096</td>\n",
       "      <td>0.132673</td>\n",
       "      <td>0.443852</td>\n",
       "      <td>0.152412</td>\n",
       "      <td>0.185972</td>\n",
       "      <td>0.256741</td>\n",
       "      <td>0.225724</td>\n",
       "      <td>0.135467</td>\n",
       "      <td>0.430226</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.936107</td>\n",
       "      <td>-1.034844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.847530</td>\n",
       "      <td>-0.850366</td>\n",
       "      <td>-0.682111</td>\n",
       "      <td>-0.688076</td>\n",
       "      <td>-0.907009</td>\n",
       "      <td>-0.874924</td>\n",
       "      <td>-0.676311</td>\n",
       "      <td>-0.291084</td>\n",
       "      <td>-0.940449</td>\n",
       "      <td>-0.864881</td>\n",
       "      <td>-0.705968</td>\n",
       "      <td>0.289241</td>\n",
       "      <td>-0.850945</td>\n",
       "      <td>-0.669577</td>\n",
       "      <td>-0.196376</td>\n",
       "      <td>-0.032696</td>\n",
       "      <td>-0.822416</td>\n",
       "      <td>-0.545392</td>\n",
       "      <td>0.097529</td>\n",
       "      <td>-0.015070</td>\n",
       "      <td>-0.789722</td>\n",
       "      <td>-0.594149</td>\n",
       "      <td>-0.059218</td>\n",
       "      <td>0.202129</td>\n",
       "      <td>-0.774599</td>\n",
       "      <td>-0.294205</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>-0.490095</td>\n",
       "      <td>-0.189547</td>\n",
       "      <td>-0.480214</td>\n",
       "      <td>0.177657</td>\n",
       "      <td>0.363962</td>\n",
       "      <td>-0.760556</td>\n",
       "      <td>-0.538654</td>\n",
       "      <td>-0.211957</td>\n",
       "      <td>-2.061689</td>\n",
       "      <td>-0.957837</td>\n",
       "      <td>-0.460555</td>\n",
       "      <td>0.251177</td>\n",
       "      <td>-0.174670</td>\n",
       "      <td>-0.850758</td>\n",
       "      <td>0.162389</td>\n",
       "      <td>0.477212</td>\n",
       "      <td>-0.661548</td>\n",
       "      <td>0.213693</td>\n",
       "      <td>-0.322219</td>\n",
       "      <td>0.464847</td>\n",
       "      <td>-0.298362</td>\n",
       "      <td>-0.692192</td>\n",
       "      <td>-0.212578</td>\n",
       "      <td>0.467568</td>\n",
       "      <td>-2.077473</td>\n",
       "      <td>-0.919472</td>\n",
       "      <td>-0.142205</td>\n",
       "      <td>0.753430</td>\n",
       "      <td>0.586777</td>\n",
       "      <td>-0.669266</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009054</td>\n",
       "      <td>-0.421326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.268225</td>\n",
       "      <td>0.301105</td>\n",
       "      <td>0.251388</td>\n",
       "      <td>-0.192742</td>\n",
       "      <td>0.430685</td>\n",
       "      <td>-0.162546</td>\n",
       "      <td>-0.459247</td>\n",
       "      <td>0.052252</td>\n",
       "      <td>0.108903</td>\n",
       "      <td>0.545458</td>\n",
       "      <td>0.219683</td>\n",
       "      <td>-0.302319</td>\n",
       "      <td>0.712037</td>\n",
       "      <td>0.536282</td>\n",
       "      <td>0.583068</td>\n",
       "      <td>0.181580</td>\n",
       "      <td>0.620642</td>\n",
       "      <td>0.704726</td>\n",
       "      <td>0.872337</td>\n",
       "      <td>0.061274</td>\n",
       "      <td>0.737832</td>\n",
       "      <td>1.340328</td>\n",
       "      <td>1.476472</td>\n",
       "      <td>-0.265677</td>\n",
       "      <td>1.282583</td>\n",
       "      <td>1.617956</td>\n",
       "      <td>1.660884</td>\n",
       "      <td>-0.241007</td>\n",
       "      <td>1.535169</td>\n",
       "      <td>1.164344</td>\n",
       "      <td>1.392298</td>\n",
       "      <td>0.100500</td>\n",
       "      <td>1.108230</td>\n",
       "      <td>1.435430</td>\n",
       "      <td>1.681006</td>\n",
       "      <td>0.334482</td>\n",
       "      <td>1.320691</td>\n",
       "      <td>1.744880</td>\n",
       "      <td>2.109632</td>\n",
       "      <td>0.333092</td>\n",
       "      <td>1.538273</td>\n",
       "      <td>0.231047</td>\n",
       "      <td>-0.212759</td>\n",
       "      <td>-0.046729</td>\n",
       "      <td>0.528159</td>\n",
       "      <td>-0.112984</td>\n",
       "      <td>-0.294142</td>\n",
       "      <td>-0.200327</td>\n",
       "      <td>0.174252</td>\n",
       "      <td>0.079560</td>\n",
       "      <td>-0.433116</td>\n",
       "      <td>-0.402624</td>\n",
       "      <td>0.429572</td>\n",
       "      <td>-0.058588</td>\n",
       "      <td>-0.518298</td>\n",
       "      <td>-0.319256</td>\n",
       "      <td>0.310258</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>5.049730</td>\n",
       "      <td>1.907380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.179965</td>\n",
       "      <td>5.018684</td>\n",
       "      <td>1.811142</td>\n",
       "      <td>-0.135763</td>\n",
       "      <td>4.915402</td>\n",
       "      <td>-0.527332</td>\n",
       "      <td>-1.879903</td>\n",
       "      <td>-0.452544</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>-0.060383</td>\n",
       "      <td>-1.671617</td>\n",
       "      <td>-0.365830</td>\n",
       "      <td>0.414717</td>\n",
       "      <td>0.221394</td>\n",
       "      <td>-1.456920</td>\n",
       "      <td>-0.577199</td>\n",
       "      <td>0.655899</td>\n",
       "      <td>0.296237</td>\n",
       "      <td>-1.435447</td>\n",
       "      <td>-0.505792</td>\n",
       "      <td>0.729455</td>\n",
       "      <td>0.367560</td>\n",
       "      <td>-1.302534</td>\n",
       "      <td>-0.581956</td>\n",
       "      <td>0.791469</td>\n",
       "      <td>0.358321</td>\n",
       "      <td>-1.366354</td>\n",
       "      <td>-0.506919</td>\n",
       "      <td>0.799537</td>\n",
       "      <td>0.577591</td>\n",
       "      <td>-1.145968</td>\n",
       "      <td>-0.551027</td>\n",
       "      <td>0.992848</td>\n",
       "      <td>0.811802</td>\n",
       "      <td>-1.087003</td>\n",
       "      <td>-0.511378</td>\n",
       "      <td>1.218166</td>\n",
       "      <td>0.395159</td>\n",
       "      <td>-1.204898</td>\n",
       "      <td>-0.167806</td>\n",
       "      <td>0.853551</td>\n",
       "      <td>0.077131</td>\n",
       "      <td>-1.320655</td>\n",
       "      <td>-0.261668</td>\n",
       "      <td>0.577012</td>\n",
       "      <td>-0.375488</td>\n",
       "      <td>-1.424007</td>\n",
       "      <td>-0.440025</td>\n",
       "      <td>0.158792</td>\n",
       "      <td>-0.229342</td>\n",
       "      <td>-1.542217</td>\n",
       "      <td>-0.422333</td>\n",
       "      <td>0.315780</td>\n",
       "      <td>-0.475493</td>\n",
       "      <td>-1.488793</td>\n",
       "      <td>-0.227721</td>\n",
       "      <td>0.081928</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>-0.213581</td>\n",
       "      <td>-0.687984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>-0.381864</td>\n",
       "      <td>-0.183234</td>\n",
       "      <td>-0.350811</td>\n",
       "      <td>-0.277247</td>\n",
       "      <td>-0.459469</td>\n",
       "      <td>-0.749584</td>\n",
       "      <td>-1.377892</td>\n",
       "      <td>-0.167903</td>\n",
       "      <td>-0.164544</td>\n",
       "      <td>-0.813724</td>\n",
       "      <td>-0.361356</td>\n",
       "      <td>0.189887</td>\n",
       "      <td>-0.625225</td>\n",
       "      <td>-0.443081</td>\n",
       "      <td>-0.304225</td>\n",
       "      <td>-0.484863</td>\n",
       "      <td>-0.291659</td>\n",
       "      <td>-0.970622</td>\n",
       "      <td>-0.261229</td>\n",
       "      <td>0.101828</td>\n",
       "      <td>-0.087610</td>\n",
       "      <td>-0.691720</td>\n",
       "      <td>-0.322433</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>-0.177914</td>\n",
       "      <td>-0.583249</td>\n",
       "      <td>-1.557905</td>\n",
       "      <td>0.103523</td>\n",
       "      <td>-0.492231</td>\n",
       "      <td>-0.421583</td>\n",
       "      <td>0.094546</td>\n",
       "      <td>-0.275068</td>\n",
       "      <td>-0.231262</td>\n",
       "      <td>-0.312844</td>\n",
       "      <td>-1.483041</td>\n",
       "      <td>-0.029100</td>\n",
       "      <td>-0.055719</td>\n",
       "      <td>-0.040806</td>\n",
       "      <td>-1.297301</td>\n",
       "      <td>0.108705</td>\n",
       "      <td>0.192763</td>\n",
       "      <td>0.203655</td>\n",
       "      <td>-1.289285</td>\n",
       "      <td>0.328461</td>\n",
       "      <td>-0.106628</td>\n",
       "      <td>0.513221</td>\n",
       "      <td>0.079782</td>\n",
       "      <td>-0.210005</td>\n",
       "      <td>-0.051257</td>\n",
       "      <td>0.482025</td>\n",
       "      <td>-0.136004</td>\n",
       "      <td>-0.097547</td>\n",
       "      <td>0.356991</td>\n",
       "      <td>0.500995</td>\n",
       "      <td>-1.656516</td>\n",
       "      <td>0.386717</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>-0.630908</td>\n",
       "      <td>-0.385557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.757292</td>\n",
       "      <td>-0.156829</td>\n",
       "      <td>0.894009</td>\n",
       "      <td>-0.763974</td>\n",
       "      <td>-0.732295</td>\n",
       "      <td>-0.404007</td>\n",
       "      <td>0.459249</td>\n",
       "      <td>-0.664043</td>\n",
       "      <td>-0.872951</td>\n",
       "      <td>-0.630503</td>\n",
       "      <td>-0.591045</td>\n",
       "      <td>2.378553</td>\n",
       "      <td>-0.754973</td>\n",
       "      <td>-0.419584</td>\n",
       "      <td>-0.115546</td>\n",
       "      <td>2.670741</td>\n",
       "      <td>-0.763314</td>\n",
       "      <td>-0.342474</td>\n",
       "      <td>-0.018954</td>\n",
       "      <td>2.654365</td>\n",
       "      <td>-0.663676</td>\n",
       "      <td>-0.403781</td>\n",
       "      <td>0.437635</td>\n",
       "      <td>-0.265455</td>\n",
       "      <td>-0.770696</td>\n",
       "      <td>-0.325835</td>\n",
       "      <td>0.585436</td>\n",
       "      <td>-0.243274</td>\n",
       "      <td>-0.721821</td>\n",
       "      <td>-0.639511</td>\n",
       "      <td>-0.169543</td>\n",
       "      <td>-0.201878</td>\n",
       "      <td>-0.776451</td>\n",
       "      <td>-0.307266</td>\n",
       "      <td>-0.423781</td>\n",
       "      <td>1.567358</td>\n",
       "      <td>-0.153830</td>\n",
       "      <td>-0.719972</td>\n",
       "      <td>-0.696778</td>\n",
       "      <td>1.476752</td>\n",
       "      <td>-0.781090</td>\n",
       "      <td>-0.642681</td>\n",
       "      <td>-0.278653</td>\n",
       "      <td>-1.214904</td>\n",
       "      <td>-0.807661</td>\n",
       "      <td>-0.568881</td>\n",
       "      <td>-0.139472</td>\n",
       "      <td>0.710012</td>\n",
       "      <td>-0.727881</td>\n",
       "      <td>-0.493411</td>\n",
       "      <td>-0.446525</td>\n",
       "      <td>2.109666</td>\n",
       "      <td>-0.765307</td>\n",
       "      <td>-0.665143</td>\n",
       "      <td>-0.351103</td>\n",
       "      <td>-1.270820</td>\n",
       "      <td>-0.866452</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>-0.770931</td>\n",
       "      <td>-0.730656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.706858</td>\n",
       "      <td>-0.755810</td>\n",
       "      <td>-0.572213</td>\n",
       "      <td>-0.730156</td>\n",
       "      <td>-0.700800</td>\n",
       "      <td>-0.389091</td>\n",
       "      <td>-0.371127</td>\n",
       "      <td>0.797740</td>\n",
       "      <td>-0.210322</td>\n",
       "      <td>-0.710690</td>\n",
       "      <td>-0.370031</td>\n",
       "      <td>-0.309427</td>\n",
       "      <td>-0.726377</td>\n",
       "      <td>-0.295688</td>\n",
       "      <td>-0.204278</td>\n",
       "      <td>0.713678</td>\n",
       "      <td>-0.140739</td>\n",
       "      <td>-0.374787</td>\n",
       "      <td>0.442766</td>\n",
       "      <td>-0.293809</td>\n",
       "      <td>-0.634493</td>\n",
       "      <td>-0.225889</td>\n",
       "      <td>0.758425</td>\n",
       "      <td>-0.324618</td>\n",
       "      <td>-0.568363</td>\n",
       "      <td>-0.061772</td>\n",
       "      <td>0.976851</td>\n",
       "      <td>-1.810383</td>\n",
       "      <td>-0.538070</td>\n",
       "      <td>-0.356982</td>\n",
       "      <td>0.347175</td>\n",
       "      <td>0.374910</td>\n",
       "      <td>-0.563450</td>\n",
       "      <td>-0.191770</td>\n",
       "      <td>0.747478</td>\n",
       "      <td>-0.347358</td>\n",
       "      <td>-0.552042</td>\n",
       "      <td>-0.216048</td>\n",
       "      <td>0.599155</td>\n",
       "      <td>-1.135824</td>\n",
       "      <td>-0.548237</td>\n",
       "      <td>0.292473</td>\n",
       "      <td>0.511941</td>\n",
       "      <td>1.063620</td>\n",
       "      <td>0.321822</td>\n",
       "      <td>-0.182030</td>\n",
       "      <td>0.726872</td>\n",
       "      <td>-0.286856</td>\n",
       "      <td>-0.606321</td>\n",
       "      <td>0.204180</td>\n",
       "      <td>0.410244</td>\n",
       "      <td>2.165071</td>\n",
       "      <td>0.092835</td>\n",
       "      <td>0.038830</td>\n",
       "      <td>1.058540</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.451277</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>-0.270461</td>\n",
       "      <td>-0.914837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074220</td>\n",
       "      <td>1.380377</td>\n",
       "      <td>-0.192961</td>\n",
       "      <td>1.025925</td>\n",
       "      <td>1.628564</td>\n",
       "      <td>2.049637</td>\n",
       "      <td>1.116960</td>\n",
       "      <td>-0.971175</td>\n",
       "      <td>2.067650</td>\n",
       "      <td>2.408283</td>\n",
       "      <td>1.487522</td>\n",
       "      <td>3.031487</td>\n",
       "      <td>2.290183</td>\n",
       "      <td>2.793412</td>\n",
       "      <td>2.184921</td>\n",
       "      <td>-0.213248</td>\n",
       "      <td>2.618692</td>\n",
       "      <td>2.583219</td>\n",
       "      <td>1.317806</td>\n",
       "      <td>4.197017</td>\n",
       "      <td>2.411010</td>\n",
       "      <td>2.287608</td>\n",
       "      <td>1.466761</td>\n",
       "      <td>-0.518084</td>\n",
       "      <td>2.249048</td>\n",
       "      <td>0.225287</td>\n",
       "      <td>-0.902333</td>\n",
       "      <td>1.529071</td>\n",
       "      <td>0.560770</td>\n",
       "      <td>0.409122</td>\n",
       "      <td>-0.513824</td>\n",
       "      <td>0.120492</td>\n",
       "      <td>0.745911</td>\n",
       "      <td>0.306026</td>\n",
       "      <td>-0.379953</td>\n",
       "      <td>0.053483</td>\n",
       "      <td>0.627628</td>\n",
       "      <td>0.372378</td>\n",
       "      <td>-0.913971</td>\n",
       "      <td>1.523638</td>\n",
       "      <td>0.730686</td>\n",
       "      <td>1.102452</td>\n",
       "      <td>-0.510751</td>\n",
       "      <td>0.531093</td>\n",
       "      <td>1.444493</td>\n",
       "      <td>0.455569</td>\n",
       "      <td>-0.587092</td>\n",
       "      <td>1.455381</td>\n",
       "      <td>0.779486</td>\n",
       "      <td>0.571168</td>\n",
       "      <td>-0.228176</td>\n",
       "      <td>-0.477693</td>\n",
       "      <td>0.894841</td>\n",
       "      <td>0.484120</td>\n",
       "      <td>-0.591427</td>\n",
       "      <td>1.583590</td>\n",
       "      <td>0.810209</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0_e      0_px  0_py      0_pz       1_e      1_px      1_py      1_pz       2_e  ...     13_px     13_py     13_pz      14_e     14_px     14_py     14_pz  gen_target  gen_weight\n",
       "0     -0.481700 -0.132916   0.0 -0.609365 -0.582372 -0.205052 -0.638221 -0.631650 -0.738738  ...  0.813110 -0.081188 -0.433435  0.065368  1.084211 -0.184324 -0.407374           0        None\n",
       "1     -0.506164 -0.938812   0.0 -0.184967  0.728438 -0.282945 -0.147106  1.005388  1.254904  ...  0.424342 -0.276123  1.008659  0.593994  0.253645 -0.251510  0.808679           0        None\n",
       "2     -0.408961 -0.560417   0.0 -0.193791 -0.468981 -0.675213 -0.255126 -0.189504  0.074884  ...  0.443852  0.152412  0.185972  0.256741  0.225724  0.135467  0.430226           0        None\n",
       "3     -0.936107 -1.034844   0.0 -0.847530 -0.850366 -0.682111 -0.688076 -0.907009 -0.874924  ...  0.467568 -2.077473 -0.919472 -0.142205  0.753430  0.586777 -0.669266           0        None\n",
       "4      0.009054 -0.421326   0.0  0.268225  0.301105  0.251388 -0.192742  0.430685 -0.162546  ... -0.433116 -0.402624  0.429572 -0.058588 -0.518298 -0.319256  0.310258           0        None\n",
       "...         ...       ...   ...       ...       ...       ...       ...       ...       ...  ...       ...       ...       ...       ...       ...       ...       ...         ...         ...\n",
       "99995  5.049730  1.907380   0.0  5.179965  5.018684  1.811142 -0.135763  4.915402 -0.527332  ... -1.542217 -0.422333  0.315780 -0.475493 -1.488793 -0.227721  0.081928           0        None\n",
       "99996 -0.213581 -0.687984   0.0  0.083984 -0.381864 -0.183234 -0.350811 -0.277247 -0.459469  ...  0.482025 -0.136004 -0.097547  0.356991  0.500995 -1.656516  0.386717           1        None\n",
       "99997 -0.630908 -0.385557   0.0 -0.757292 -0.156829  0.894009 -0.763974 -0.732295 -0.404007  ... -0.446525  2.109666 -0.765307 -0.665143 -0.351103 -1.270820 -0.866452           1        None\n",
       "99998 -0.770931 -0.730656   0.0 -0.706858 -0.755810 -0.572213 -0.730156 -0.700800 -0.389091  ...  0.410244  2.165071  0.092835  0.038830  1.058540 -0.323477 -0.451277           1        None\n",
       "99999 -0.270461 -0.914837   0.0  0.074220  1.380377 -0.192961  1.025925  1.628564  2.049637  ... -0.228176 -0.477693  0.894841  0.484120 -0.591427  1.583590  0.810209           1        None\n",
       "\n",
       "[100000 rows x 62 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fy.get_df(inc_inputs=True, suppress_warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "We'll be trying out several different models, but in each case we'll mostly be changing the `head` of the model, so let's write a function to return a suitable `ModelBuilder` from a few arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumin.nn.models.model_builder import ModelBuilder\n",
    "from lumin.nn.models.model import Model\n",
    "from lumin.nn.models.blocks.body import FullyConnected\n",
    "from lumin.nn.models.helpers import CatEmbedder\n",
    "from lumin.nn.models.blocks.head import AbsHead\n",
    "from typing import *\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bs = 256\n",
    "objective = 'classifier'\n",
    "n_out = 1\n",
    "\n",
    "def get_model_builder(fy:FoldYielder, depth:int, width:int, head:AbsHead=CatEmbHead) -> ModelBuilder:\n",
    "    opt_args = {'opt':'adam', 'eps':1e-08}\n",
    "    cat_embedder = CatEmbedder.from_fy(fy)\n",
    "    body = partial(FullyConnected, depth=depth, width=width, act='swish')\n",
    "    model_builder = ModelBuilder(objective, cont_feats=fy.cont_feats, n_out=n_out, cat_embedder=cat_embedder, \n",
    "                                 opt_args=opt_args, body=body, head=head)\n",
    "    print(Model(model_builder))\n",
    "    return model_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN\n",
    "\n",
    "The basic approach to inputting the data is to keep the data flat and feed every single feature into the model simultaneously. Using the first 15 constituents, we have 60 input features and as we increase the number of constituents, the number of inputs would grow considerably. This means that the number of parameters in the input layer of our DNN would also grow.\n",
    "\n",
    "The aim will be to try and replace this fully-connected input layer with a more efficient layer. Nonetheless, the DNN allows all constituents to be considered simultaneously, so should theoretically offer the best performance, provided it can be trained.\n",
    "\n",
    "For our tests, we'll use a DNN with 4 hidden layers of 50 neurons, and for later examples, drop a hidden layer in favour of a different input head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "60 Continuous: ['0_e', '0_px', '0_py', '0_pz', '1_e', '1_px', '1_py', '1_pz', '2_e', '2_px', '2_py', '2_pz', '3_e', '3_px', '3_py', '3_pz', '4_e', '4_px', '4_py', '4_pz', '5_e', '5_px', '5_py', '5_pz', '6_e', '6_px', '6_py', '6_pz', '7_e', '7_px', '7_py', '7_pz', '8_e', '8_px', '8_py', '8_pz', '9_e', '9_px', '9_py', '9_pz', '10_e', '10_px', '10_py', '10_pz', '11_e', '11_px', '11_py', '11_pz', '12_e', '12_px', '12_py', '12_pz', '13_e', '13_px', '13_py', '13_pz', '14_e', '14_px', '14_py', '14_pz']\n",
      "                   \n",
      "0  Categorical: []\n",
      "                   \n",
      "0  Matrix elements: []\n",
      "                   \n",
      "\n",
      "Model:\n",
      "<bound method Module.parameters of Sequential(\n",
      "  (0): CatEmbHead()\n",
      "  (1): FullyConnected(\n",
      "    (layers): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=60, out_features=50, bias=True)\n",
      "        (1): Swish()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=50, out_features=50, bias=True)\n",
      "        (1): Swish()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Linear(in_features=50, out_features=50, bias=True)\n",
      "        (1): Swish()\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Linear(in_features=50, out_features=50, bias=True)\n",
      "        (1): Swish()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): ClassRegMulti(\n",
      "    (dense): Linear(in_features=50, out_features=1, bias=True)\n",
      "    (act): Sigmoid()\n",
      "  )\n",
      ")>\n",
      "                   \n",
      "\n",
      "Number of trainable parameters: 10751\n",
      "                   \n",
      "\n",
      "Optimiser:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.002\n",
      "    weight_decay: 0\n",
      ")\n",
      "                   \n",
      "\n",
      "Loss:\n",
      "<class 'torch.nn.modules.loss.BCELoss'>\n"
     ]
    }
   ],
   "source": [
    "model_builder = get_model_builder(train_fy, depth=4, width=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our baseline model uses 10751 parameters. Let's train it and check the score. We'll use the area under the ROC curve as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumin.optimisation.hyper_param import lr_find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder = lr_find(train_fy, model_builder, bs, lr_bounds=[1e-6,1e0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumin.nn.callbacks.cyclic_callbacks import *\n",
    "from lumin.nn.metrics.class_eval import RocAucScore, BinaryAccuracy\n",
    "from functools import partial\n",
    "\n",
    "cb_partials = [partial(OneCycle, lengths=[5, 10],lr_range=[1e-4, 1e-2], mom_range=[0.85, 0.95], interp='cosine')]\n",
    "metric_partials = [RocAucScore, BinaryAccuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumin.nn.training.train import train_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = train_models(train_fy, n_models=1,\n",
    "                 model_builder=model_builder,\n",
    "                 bs=bs,\n",
    "                 cb_partials=cb_partials,\n",
    "                 metric_partials=metric_partials,\n",
    "                 n_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our DNN achieves a validation AUC of 0.935"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gravitational Net\n",
    "We'll construct a graph for each jet by using the sub-jets as nodes, and their 4-momenta as features per node. Our data is still a flat table of features per jet, but it can automatically be reshaped into a tensor of (batch x node x feature) by defining what prefixes to consider as nodes (vectors) and what suffixes consider as features per node.\n",
    "\n",
    "E.g. given a feature name of \"0_px\", \"0\" should be considered as a node identifier, and \"px\" should considered as a feature of the \"0\" node.\n",
    "\n",
    "Note, the flat data will be reshaped during the forward pass, however we could have saved the data already processed into the (batch x node x feature) tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['px', 'py', 'pz', 'e'],\n",
       " ['0',\n",
       "  '1',\n",
       "  '2',\n",
       "  '3',\n",
       "  '4',\n",
       "  '5',\n",
       "  '6',\n",
       "  '7',\n",
       "  '8',\n",
       "  '9',\n",
       "  '10',\n",
       "  '11',\n",
       "  '12',\n",
       "  '13',\n",
       "  '14'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lumin.data_processing.hep_proc import get_vecs\n",
    "\n",
    "fpv = ['px', 'py', 'pz', 'e']\n",
    "vecs = [str(i) for i in sorted([int(f) for f in get_vecs(train_fy.cont_feats)])]\n",
    "fpv, vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNNs in LUMIN consist of two parts:\n",
    "- extractor: GNN layers used to compute new features per node\n",
    "- collapser: aggregates/flattens node features into flat vector. Can also apply a few fully connected layers to nodes prior to collapsing\n",
    "\n",
    "`GNNHead` class wraps the extractor and collapser in a single object. Forward method:\n",
    "```python\n",
    "def forward(self, x:Union[Tensor,Tuple[Tensor,Tensor]]) -> Tensor:\n",
    "    r'''\n",
    "    Passes input through the GravNet head and returns a flat tensor.\n",
    "\n",
    "    Arguments:\n",
    "        x: If a tuple, the second element is assumed to the be the matrix data. If a flat tensor, will convert the data to a matrix\n",
    "\n",
    "    Returns:\n",
    "        Resulting tensor\n",
    "    '''\n",
    "\n",
    "    x = self._process_input(x)  # features per vtx per datapoint\n",
    "    if self.use_in_bn: x = self.bn(x)\n",
    "    if self.cat_means:\n",
    "        if self.row_wise: x = torch.cat([x,x.mean(1).unsqueeze(2).repeat_interleave(repeats=x.shape[1],dim=2).transpose(1,2)],dim=2)\n",
    "        else:             x = torch.cat([x,x.mean(2).unsqueeze(1).repeat_interleave(repeats=x.shape[2],dim=1).transpose(1,2)],dim=1)\n",
    "    x = self.extractor(x)  # new features per vtx per datapoint\n",
    "    if not self.row_wise: x = x.transpose(1,2)\n",
    "    x = self.collapser(x)  # features per datapoint\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumin.nn.models.blocks.head import GNNHead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GravNet layers can be stacked together. `GravNet` class passes node features through a set number of layers, and concatenates the output of each one to the initial node features. Forward method\n",
    "```python\n",
    "def forward(self, x:Tensor) -> Tensor:\n",
    "    r'''\n",
    "    Passes input through the GravNet head.\n",
    "\n",
    "    Arguments:\n",
    "        x: row-wise tensor (batch x vertices x features)\n",
    "\n",
    "    Returns:\n",
    "        Resulting tensor row-wise tensor (batch x vertices x new features)\n",
    "    '''\n",
    "\n",
    "    outs = [x]\n",
    "    for l in self.grav_layers: outs.append(l(outs[-1]))\n",
    "    return torch.cat(outs[1:], dim=-1)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Each layer is a `GravNetLayer` class. Breaking down the forwards pass step by step:\n",
    "```python\n",
    "def forward(self, x:Tensor) -> Tensor:\n",
    "    r'''\n",
    "    Pass batch of vertices through GravNet layer and return new features per vertex\n",
    "\n",
    "    Arguments:\n",
    "        x: Incoming data (batch x vertices x features)\n",
    "\n",
    "    Returns:\n",
    "        Data with new features per vertex (batch x vertices x new features)\n",
    "    '''\n",
    "\n",
    "    # Concat means\n",
    "    if self.cat_means:\n",
    "        x = torch.cat([x,x.mean(1).unsqueeze(2).repeat_interleave(repeats=x.shape[1],dim=-1).transpose(1,2)],dim=2)\n",
    "```\n",
    "The features per node can be optionally extended to include the mean values of each feature across the graph.\n",
    "```python\n",
    "    # Compute spatial and vertex features\n",
    "    slr = self.f_slr(x)\n",
    "    s,lr = slr[:,:,:self.n_s],slr[:,:,self.n_s:]\n",
    "```\n",
    "An initial DNN `f_slr` is used to compute: `s` spatial coordinates, and `lr` features per node. In PyTorch, DNNs automatically broadcast across all tensor dimensions except the last one, so `f_slr(x)` applies the same DNN to every node in every graph in the batch.\n",
    "```python\n",
    "    # kNN\n",
    "    d_jk = torch.norm(s[:,:,None]-s[:,None], dim=-1)\n",
    "    idxs = self._knn(d_jk)\n",
    "```\n",
    "For every node, compute the Euclidean distance between it and all nodes in the same graph (including itself) using `s` as coordinates in latent space. For every node, select the *k* nearest neighbours (including itself). *N.B.* \"for\" is used here to describe the process, but the above lines are vectorised and compute all distances in a single function.\n",
    "```python\n",
    "    def _knn(self, dists:Tensor) -> Tuple[Tensor,Tensor,Tensor]:\n",
    "        idxs = dists.argsort()\n",
    "        i = np.arange(dists.shape[0])[:,None,None]\n",
    "        j = np.arange(dists.shape[1])[None,:,None]\n",
    "        return i,j,idxs[:,:,:self.k]\n",
    "```\n",
    "This just orders the distances, and returns indices to look up the *k* nearest neighbours.\n",
    "```python\n",
    "    d_jk = d_jk[idxs] \n",
    "    lr = lr[:,None].expand(lr.shape[0],lr.shape[1],lr.shape[1],lr.shape[2])[idxs]\n",
    "```\n",
    "The distances `d_jk` are already a (batch x node x node) tensor, and are indexed to return a (batch x node x *k*-nearest nodes) tensor.  The `lr` features are expanded and indexed into a (batch x node x *k*-nearest nodes x features).\n",
    "```python\n",
    "    v_jk = self.potential(d_jk)\n",
    "    ft_ijk = lr*v_jk.unsqueeze(-1)\n",
    "    if self.use_sa: ft_ijk = self.sa_agg(ft_ijk)\n",
    "```\n",
    "The `d_jk` distances are used to weight feature through a potential function, e.g. `torch.exp(-10*(d_jk**2)`. I.e. close-by nodes' features have a higher weight than those of more distance nodes. Optionally, and unique to LUMIN, [self-attention](https://arxiv.org/abs/1706.03762) can be used to here to also provide a learnable feature weighting (still experimental, and sometimes reduces performance).\n",
    "```python\n",
    "    fp = [x]\n",
    "    for agg in self.agg_methods: fp.append(agg(ft_ijk))\n",
    "    fp = torch.cat(fp,dim=-1)\n",
    "```\n",
    "Neighbourhhood features are constructed by aggregating the distance-weighted features of the *k* nearest nodes, e.g. taking the mean and maximum. These are then concatenated with the original features of the node.\n",
    "```python\n",
    "    # Output\n",
    "    return self.f_out(fp)\n",
    "```\n",
    "Finally, the output features per node per graph are computed using a second DNN based on the node features + the neighbourhood features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumin.nn.models.blocks.gnn_blocks import  GravNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we pass the (batch x node x features) tensor through the `GraphCollapser` to transform it into a (batch x feature) tensor:\n",
    "```python\n",
    "def forward(self, x:Tensor) -> Tensor:\n",
    "    r'''\n",
    "    Collapses features per vertex down to features\n",
    "\n",
    "    Arguemnts:\n",
    "        x: incoming data (batch x vertices x features)\n",
    "\n",
    "    Returns:\n",
    "        Flattened data (batch x flat features)\n",
    "    '''\n",
    "    if self.global_feat_vec and self.gfv_pos == 'pre-initial':\n",
    "        x = torch.cat([x,x.mean(1).unsqueeze(2).repeat_interleave(repeats=x.shape[1],dim=2).transpose(1,2)],dim=2) \n",
    "    x = self.f_inital(x)\n",
    "    if self.n_sa_layers > 0:\n",
    "        outs = []\n",
    "        for i, sa in enumerate(self.sa_layers): outs.append(sa(outs[-1] if i > 0 else x))\n",
    "        x = torch.cat(outs, dim=-1)\n",
    "    if self.global_feat_vec and self.gfv_pos == 'pre-final':\n",
    "        x = torch.cat([x,x.mean(1).unsqueeze(2).repeat_interleave(repeats=x.shape[1],dim=2).transpose(1,2)],dim=2)\n",
    "    x = self.f_final(x)\n",
    "    return self._agg(x)\n",
    "```\n",
    "This is quite configurable:\n",
    "- DNNs (`f_inital` and `f_final`) can be used to reduce the number of features per node per graph and two different stages (or they can be `lambda x: x`).\n",
    "- A \"global feature vertex\" can be computed, which concatenates the mean values of features across all nodes per graph (like the `cat_means` in the gravnet layers). This can be done before either of the two DNNs.\n",
    "- Multiple self-attention blocks can be used to augment the features, a la [point-cloud transformer](https://link.springer.com/article/10.1007/s41095-021-0229-5) (see also [Point Cloud Transformers applied to Collider Physics](https://iopscience.iop.org/article/10.1088/2632-2153/ac07f6))\n",
    "- Finally the tensor is either reshaped or aggregated:\n",
    "```python\n",
    "def _agg(self, x:Tensor) -> Tensor:\n",
    "    if self.flatten: return x.reshape((len(x),-1))\n",
    "    else:            return torch.cat([agg(x) for agg in self.agg_methods],-1)\n",
    "```\n",
    "where `agg_methods` are the mean and maximum of the features across the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "60 Continuous: ['0_e', '0_px', '0_py', '0_pz', '1_e', '1_px', '1_py', '1_pz', '2_e', '2_px', '2_py', '2_pz', '3_e', '3_px', '3_py', '3_pz', '4_e', '4_px', '4_py', '4_pz', '5_e', '5_px', '5_py', '5_pz', '6_e', '6_px', '6_py', '6_pz', '7_e', '7_px', '7_py', '7_pz', '8_e', '8_px', '8_py', '8_pz', '9_e', '9_px', '9_py', '9_pz', '10_e', '10_px', '10_py', '10_pz', '11_e', '11_px', '11_py', '11_pz', '12_e', '12_px', '12_py', '12_pz', '13_e', '13_px', '13_py', '13_pz', '14_e', '14_px', '14_py', '14_pz']\n",
      "                   \n",
      "0  Categorical: []\n",
      "                   \n",
      "0  Matrix elements: []\n",
      "                   \n",
      "\n",
      "Model:\n",
      "<bound method Module.parameters of Sequential(\n",
      "  (0): GNNHead(\n",
      "    (extractor): GravNet(\n",
      "      (grav_layers): ModuleList(\n",
      "        (0): GravNetLayer(\n",
      "          (f_slr): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=8, out_features=30, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Linear(in_features=30, out_features=30, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Linear(in_features=30, out_features=26, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "          )\n",
      "          (f_out): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=52, out_features=48, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): GravNetLayer(\n",
      "          (f_slr): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=96, out_features=30, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Linear(in_features=30, out_features=30, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Linear(in_features=30, out_features=26, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "          )\n",
      "          (f_out): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=140, out_features=48, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): GravNetLayer(\n",
      "          (f_slr): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=96, out_features=30, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Linear(in_features=30, out_features=30, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Linear(in_features=30, out_features=26, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "          )\n",
      "          (f_out): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=140, out_features=48, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): GravNetLayer(\n",
      "          (f_slr): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=96, out_features=30, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Linear(in_features=30, out_features=30, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Linear(in_features=30, out_features=26, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "          )\n",
      "          (f_out): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=140, out_features=48, bias=True)\n",
      "              (1): Swish()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (collapser): GraphCollapser(\n",
      "      (f_final): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=192, out_features=40, bias=True)\n",
      "          (1): Swish()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): FullyConnected(\n",
      "    (layers): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=600, out_features=50, bias=True)\n",
      "        (1): Swish()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=50, out_features=50, bias=True)\n",
      "        (1): Swish()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Linear(in_features=50, out_features=50, bias=True)\n",
      "        (1): Swish()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): ClassRegMulti(\n",
      "    (dense): Linear(in_features=50, out_features=1, bias=True)\n",
      "    (act): Sigmoid()\n",
      "  )\n",
      ")>\n",
      "                   \n",
      "\n",
      "Number of trainable parameters: 81713\n",
      "                   \n",
      "\n",
      "Optimiser:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.002\n",
      "    weight_decay: 0\n",
      ")\n",
      "                   \n",
      "\n",
      "Loss:\n",
      "<class 'torch.nn.modules.loss.BCELoss'>\n"
     ]
    }
   ],
   "source": [
    "from lumin.nn.models.blocks.gnn_blocks import  GravNet, GraphCollapser\n",
    "\n",
    "head = partial(GNNHead, vecs=vecs, feats_per_vec=fpv,\n",
    "               extractor=partial(GravNet,  # Use GravNet for feature extraction\n",
    "                                 cat_means=True,  # Concatenate the means of features as new features\n",
    "                                 f_slr_depth=3,  # Use 3 hidden layers to compute the coordinates and features in latent space\n",
    "                                 n_s=4,  # Use 4 dimensions for latent space\n",
    "                                 n_lr=22,  # Compute 22 features per vertex for the latent representation\n",
    "                                 k=10,  # Each vertex should consider the 10 nearest vertices (self included) when computing neighbourhood features\n",
    "                                 f_out_depth=1,  # Use one hidden layer to compute the new features per vertex\n",
    "                                 n_out=[48,48,48,48],  # Use 4 GravNet layers, each computing 48 new features per vertex\n",
    "                                 act='swish'),  # Use swish activation functions\n",
    "               collapser=partial(GraphCollapser,  # Use default graph collapser\n",
    "                                 flatten=True,  # Reshape the data into (batch x features) without aggregation. Vertices are ordered by pT\n",
    "                                 f_final_outs=[40],  # Compute 40 features per vertex based on the GravNet layer outputs\n",
    "                                 act='swish'))  # Use swish activation functions\n",
    "model_builder = get_model_builder(train_fy, depth=3, width=50, head=head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder = lr_find(train_fy, model_builder, bs, lr_bounds=[1e-6,1e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_partials[0].keywords['lr_range'] = [1e-4, 1e-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = train_models(train_fy, n_models=1,\n",
    "                 model_builder=model_builder,\n",
    "                 bs=bs,\n",
    "                 cb_partials=cb_partials,\n",
    "                 metric_partials=metric_partials,\n",
    "                 n_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ROC AUC of 0.970 is the best so far! Although we are using a very large model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
